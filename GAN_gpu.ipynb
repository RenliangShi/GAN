{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.parallel\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# download CIFAR10 train_dataset\n",
    "train_dataset = dset.CIFAR10(root='dataset', \n",
    "                             train=True, \n",
    "                             transform=transforms.Compose([\n",
    "                                 transforms.Scale(64), # This transform is deprecated in favor of Resize.\n",
    "                                 transforms.CenterCrop(64), # Crops the given PIL Image at the center.\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), \n",
    "                               ]),\n",
    "                            download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the parameter\n",
    "batchSize = 256\n",
    "imageSize = 64\n",
    "beta1 = 0.5\n",
    "lr = 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combines a dataset and a sampler, and provides single- or multi-process iterators over the dataset.\n",
    "dataloader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batchSize,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the path to the output files\n",
    "if not os.path.exists('outpath'):\n",
    "    os.mkdir('outpath')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "manualSeed = int()\n",
    "\n",
    "if manualSeed is None:\n",
    "    manualSeed = random.randint(1, 10000)\n",
    "\n",
    "random.seed(manualSeed)\n",
    "\n",
    "# Sets the seed for generating random numbers. And returns a torch._C.Generator object.\n",
    "torch.cuda.manual_seed(manualSeed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize the weights and bias\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nv = 100 # noise vector\n",
    "nc = 3 # number of chanels\n",
    "netG = '' # the generate network model\n",
    "netD = '' # the discrimitive network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _netG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netG, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z(noise vector), give to a convolution\n",
    "            nn.ConvTranspose2d( nv, 64 * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(64 * 8),\n",
    "            nn.ReLU(True),\n",
    "            # the output of the previous layer as the input\n",
    "            nn.ConvTranspose2d(64 * 8, 64 * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64 * 4),\n",
    "            nn.ReLU(True),\n",
    "            # the output of the previous layer as the input\n",
    "            nn.ConvTranspose2d(64 * 4, 64 * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64 * 2),\n",
    "            nn.ReLU(True),\n",
    "            # the output of the previous layer as the input\n",
    "            nn.ConvTranspose2d(64 * 2, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            # the output of the previous layer as the input\n",
    "            nn.ConvTranspose2d(64, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input_value):\n",
    "        output = nn.parallel.data_parallel(self.main, input_value)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_netG (\n",
       "  (main): Sequential (\n",
       "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (2): ReLU (inplace)\n",
       "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (5): ReLU (inplace)\n",
       "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (8): ReLU (inplace)\n",
       "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (11): ReLU (inplace)\n",
       "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (13): Tanh ()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate network\n",
    "netG = _netG()\n",
    "netG.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _netD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netD, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, 64, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # the output of the previous layer as the input\n",
    "            nn.Conv2d(64, 64 * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64 * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # the output of the previous layer as the input\n",
    "            nn.Conv2d(64 * 2, 64 * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64 * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # the output of the previous layer as the input\n",
    "            nn.Conv2d(64 * 4, 64 * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64 * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (64*8) x 4 x 4\n",
    "            nn.Conv2d(64 * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input_value):\n",
    "        output = nn.parallel.data_parallel(self.main, input_value)\n",
    "        return output.view(-1, 1).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_netD (\n",
       "  (main): Sequential (\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): LeakyReLU (0.2, inplace)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (4): LeakyReLU (0.2, inplace)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (7): LeakyReLU (0.2, inplace)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (10): LeakyReLU (0.2, inplace)\n",
       "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (12): Sigmoid ()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the discrimitive network model\n",
    "netD = _netD()\n",
    "netD.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the loss function\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_data = torch.FloatTensor(batchSize, 3, imageSize, imageSize) # define the shape of the input_data\n",
    "noise = torch.FloatTensor(batchSize, nv, 1, 1) # define the shape of the noise vector\n",
    "fixed_noise = torch.FloatTensor(batchSize, nv, 1, 1).normal_(0, 1)\n",
    "label = torch.FloatTensor(batchSize) # define the label\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "iteration = 20\n",
    "\n",
    "\n",
    "netD.cuda()\n",
    "netG.cuda()\n",
    "criterion.cuda()\n",
    "input_data, label = input_data.cuda(), label.cuda()\n",
    "noise, fixed_noise = noise.cuda(), fixed_noise.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fixed_noise = Variable(fixed_noise) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "# train the both of the two models\n",
    "for epoch in range(iteration):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        \n",
    "        netD.zero_grad() # Clears the gradients of all optimized Variable s.\n",
    "        train_value, _ = data # get the train value\n",
    "        batch_size = train_value.size(0) # get the batch size\n",
    "        train_value = train_value.cuda()\n",
    "        input_data.resize_as_(train_value).copy_(train_value) # resize the input size to fit the train value size\n",
    "        label.resize_(batch_size).fill_(real_label) # resize the labels\n",
    "        inputv = Variable(input_data) # the input vector\n",
    "        labelv = Variable(label) # the output vector\n",
    "        \n",
    "        \n",
    "        output = netD(inputv) # prediction of the discrimitive model on real sample\n",
    "        errD_real = criterion(output, labelv) # errors of discrimitive model on real sample\n",
    "        errD_real.backward()\n",
    "        D_x = output.data.mean()\n",
    "        \n",
    "        \n",
    "        noise.resize_(batch_size, nv, 1, 1).normal_(0, 1) # resize the noise vector\n",
    "        noisev = Variable(noise)\n",
    "        fake = netG(noisev) # prediction of the discrimitive model on fake sample\n",
    "        labelv = Variable(label.fill_(fake_label)) \n",
    "        output = netD(fake.detach())\n",
    "        errD_fake = criterion(output, labelv) # errors of discrimitive model on real sample\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.data.mean()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "        \n",
    "        netG.zero_grad() \n",
    "        labelv = Variable(label.fill_(real_label))  # fake labels are real for generator cost\n",
    "        output = netD(fake)\n",
    "        errG = criterion(output, labelv)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.data.mean()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        \n",
    "        if i % 98 == 0:\n",
    "            vutils.save_image(train_value,\n",
    "                    '%s/real_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/20][0/196] Loss_D: 1.6441 Loss_G: 6.1096 D(x): 0.4856 D(G(z)): 0.5075 / 0.0034  in 1.04 sec.\n",
      "[0/20][98/196] Loss_D: 0.0775 Loss_G: 13.0618 D(x): 0.9417 D(G(z)): 0.0000 / 0.0000  in 18.11 sec.\n",
      "[1/20][0/196] Loss_D: 0.1672 Loss_G: 4.7693 D(x): 0.8964 D(G(z)): 0.0455 / 0.0107  in 18.08 sec.\n",
      "[1/20][98/196] Loss_D: 0.2857 Loss_G: 8.1505 D(x): 0.9079 D(G(z)): 0.1491 / 0.0005  in 18.29 sec.\n",
      "[2/20][0/196] Loss_D: 0.4511 Loss_G: 4.7652 D(x): 0.8725 D(G(z)): 0.2361 / 0.0122  in 18.29 sec.\n",
      "[2/20][98/196] Loss_D: 0.4155 Loss_G: 4.2325 D(x): 0.7086 D(G(z)): 0.0078 / 0.0188  in 18.35 sec.\n",
      "[3/20][0/196] Loss_D: 0.6597 Loss_G: 5.2013 D(x): 0.9223 D(G(z)): 0.3773 / 0.0089  in 18.16 sec.\n",
      "[3/20][98/196] Loss_D: 4.4037 Loss_G: 2.3232 D(x): 0.0464 D(G(z)): 0.0006 / 0.1605  in 18.30 sec.\n",
      "[4/20][0/196] Loss_D: 1.0470 Loss_G: 9.6391 D(x): 0.9329 D(G(z)): 0.5507 / 0.0002  in 18.21 sec.\n",
      "[4/20][98/196] Loss_D: 0.4392 Loss_G: 3.5907 D(x): 0.8473 D(G(z)): 0.2132 / 0.0418  in 18.31 sec.\n",
      "[5/20][0/196] Loss_D: 1.2335 Loss_G: 4.4513 D(x): 0.8711 D(G(z)): 0.5897 / 0.0176  in 18.23 sec.\n",
      "[5/20][98/196] Loss_D: 0.3708 Loss_G: 2.8005 D(x): 0.8280 D(G(z)): 0.1417 / 0.0813  in 18.38 sec.\n",
      "[6/20][0/196] Loss_D: 0.3157 Loss_G: 3.1916 D(x): 0.8550 D(G(z)): 0.1313 / 0.0537  in 18.26 sec.\n",
      "[6/20][98/196] Loss_D: 0.4831 Loss_G: 2.4355 D(x): 0.7519 D(G(z)): 0.1441 / 0.1202  in 18.40 sec.\n",
      "[7/20][0/196] Loss_D: 0.5415 Loss_G: 3.7826 D(x): 0.8486 D(G(z)): 0.2754 / 0.0346  in 18.26 sec.\n",
      "[7/20][98/196] Loss_D: 0.8210 Loss_G: 4.0424 D(x): 0.8761 D(G(z)): 0.4291 / 0.0288  in 18.60 sec.\n",
      "[8/20][0/196] Loss_D: 0.7085 Loss_G: 3.4743 D(x): 0.8344 D(G(z)): 0.3686 / 0.0456  in 18.45 sec.\n",
      "[8/20][98/196] Loss_D: 0.5707 Loss_G: 2.5677 D(x): 0.7385 D(G(z)): 0.1877 / 0.1011  in 18.44 sec.\n",
      "[9/20][0/196] Loss_D: 0.4302 Loss_G: 1.8948 D(x): 0.7392 D(G(z)): 0.0800 / 0.1846  in 18.47 sec.\n",
      "[9/20][98/196] Loss_D: 1.0588 Loss_G: 1.2642 D(x): 0.4502 D(G(z)): 0.0506 / 0.3424  in 18.60 sec.\n",
      "[10/20][0/196] Loss_D: 0.4822 Loss_G: 3.8428 D(x): 0.8318 D(G(z)): 0.2241 / 0.0317  in 18.49 sec.\n",
      "[10/20][98/196] Loss_D: 0.9124 Loss_G: 4.5478 D(x): 0.9282 D(G(z)): 0.5259 / 0.0157  in 18.55 sec.\n",
      "[11/20][0/196] Loss_D: 0.6157 Loss_G: 3.3795 D(x): 0.8942 D(G(z)): 0.3580 / 0.0477  in 18.42 sec.\n",
      "[11/20][98/196] Loss_D: 0.8034 Loss_G: 2.2195 D(x): 0.5571 D(G(z)): 0.0899 / 0.1533  in 18.49 sec.\n",
      "[12/20][0/196] Loss_D: 1.1011 Loss_G: 2.7125 D(x): 0.7039 D(G(z)): 0.4382 / 0.0929  in 18.43 sec.\n",
      "[12/20][98/196] Loss_D: 1.4185 Loss_G: 5.6225 D(x): 0.9285 D(G(z)): 0.6756 / 0.0053  in 18.48 sec.\n",
      "[13/20][0/196] Loss_D: 1.0916 Loss_G: 1.1491 D(x): 0.4117 D(G(z)): 0.0708 / 0.3734  in 18.40 sec.\n",
      "[13/20][98/196] Loss_D: 0.5681 Loss_G: 3.9500 D(x): 0.8930 D(G(z)): 0.3272 / 0.0313  in 18.49 sec.\n",
      "[14/20][0/196] Loss_D: 0.4463 Loss_G: 3.0898 D(x): 0.8948 D(G(z)): 0.2621 / 0.0608  in 18.40 sec.\n",
      "[14/20][98/196] Loss_D: 1.2613 Loss_G: 1.7080 D(x): 0.5361 D(G(z)): 0.3566 / 0.2339  in 18.87 sec.\n",
      "[15/20][0/196] Loss_D: 0.9366 Loss_G: 1.8246 D(x): 0.4585 D(G(z)): 0.0450 / 0.2011  in 18.39 sec.\n",
      "[15/20][98/196] Loss_D: 0.4037 Loss_G: 3.0599 D(x): 0.8483 D(G(z)): 0.1930 / 0.0629  in 18.63 sec.\n",
      "[16/20][0/196] Loss_D: 1.0312 Loss_G: 1.0878 D(x): 0.4171 D(G(z)): 0.0442 / 0.3784  in 18.62 sec.\n",
      "[16/20][98/196] Loss_D: 1.0414 Loss_G: 1.6396 D(x): 0.5257 D(G(z)): 0.1953 / 0.2380  in 18.50 sec.\n",
      "[17/20][0/196] Loss_D: 0.5766 Loss_G: 2.2950 D(x): 0.6141 D(G(z)): 0.0479 / 0.1253  in 18.45 sec.\n",
      "[17/20][98/196] Loss_D: 0.4428 Loss_G: 2.6818 D(x): 0.8379 D(G(z)): 0.2146 / 0.0856  in 18.78 sec.\n",
      "[18/20][0/196] Loss_D: 1.0018 Loss_G: 2.0933 D(x): 0.6708 D(G(z)): 0.3901 / 0.1492  in 18.47 sec.\n",
      "[18/20][98/196] Loss_D: 0.3999 Loss_G: 2.9131 D(x): 0.8037 D(G(z)): 0.1470 / 0.0746  in 18.56 sec.\n",
      "[19/20][0/196] Loss_D: 0.9418 Loss_G: 1.4436 D(x): 0.5769 D(G(z)): 0.2539 / 0.2710  in 18.52 sec.\n",
      "[19/20][98/196] Loss_D: 0.3216 Loss_G: 3.6432 D(x): 0.9302 D(G(z)): 0.2085 / 0.0341  in 18.45 sec.\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "# train the both of the two models\n",
    "for epoch in range(iteration):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        \n",
    "        netD.zero_grad() # Clears the gradients of all optimized Variable s.\n",
    "        train_value, _ = data # get the train value\n",
    "        batch_size = train_value.size(0) # get the batch size\n",
    "        train_value = train_value.cuda()\n",
    "        input_data.resize_as_(train_value).copy_(train_value) # resize the input size to fit the train value size\n",
    "        label.resize_(batch_size).fill_(real_label) # resize the labels\n",
    "        inputv = Variable(input_data) # the input vector\n",
    "        labelv = Variable(label) # the output vector\n",
    "        \n",
    "        \n",
    "        output = netD(inputv) # prediction of the discrimitive model on real sample\n",
    "        errD_real = criterion(output, labelv) # errors of discrimitive model on real sample\n",
    "        errD_real.backward()\n",
    "        D_x = output.data.mean()\n",
    "        \n",
    "        \n",
    "        noise.resize_(batch_size, nv, 1, 1).normal_(0, 1) # resize the noise vector\n",
    "        noisev = Variable(noise)\n",
    "        fake = netG(noisev) # prediction of the discrimitive model on fake sample\n",
    "        labelv = Variable(label.fill_(fake_label)) \n",
    "        output = netD(fake.detach())\n",
    "        errD_fake = criterion(output, labelv) # errors of discrimitive model on real sample\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.data.mean()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "        \n",
    "        netG.zero_grad() \n",
    "        labelv = Variable(label.fill_(real_label))  # fake labels are real for generator cost\n",
    "        output = netD(fake)\n",
    "        errG = criterion(output, labelv)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.data.mean()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        \n",
    "        if i % 98 == 0:\n",
    "            vutils.save_image(train_value,\n",
    "                    '%s/real_samples_epoch_%03d.png' % ('outpath', epoch),\n",
    "                    normalize=True)\n",
    "            fake = netG(fixed_noise)\n",
    "            vutils.save_image(fake.data,\n",
    "                    '%s/fake_samples_epoch_%03d.png' % ('outpath', epoch),\n",
    "                    normalize=True)\n",
    "            t2 = time.time()\n",
    "            dt = t2 - t1\n",
    "            t1 = t2\n",
    "            print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f  in %.2f sec.'\n",
    "                  % (epoch, iteration, i, len(dataloader),\n",
    "                     errD.data[0], errG.data[0], D_x, D_G_z1, D_G_z2, dt))\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "#         t2 = time.time()\n",
    "#         if i % 98 == 0:\n",
    "#             dt = t2 - t1\n",
    "#             print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f  in %.2f sec.'\n",
    "#                   % (epoch, iteration, i, len(dataloader),\n",
    "#                      errD.data[0], errG.data[0], D_x, D_G_z1, D_G_z2, dt))\n",
    "        \n",
    "#         if i % 98 == 0:\n",
    "#             vutils.save_image(train_value,\n",
    "#                     '%s/real_samples.png' % 'outpath',\n",
    "#                     normalize=True)\n",
    "#             fake = netG(fixed_noise)\n",
    "#             vutils.save_image(fake.data,\n",
    "#                     '%s/fake_samples_epoch_%03d.png' % ('outpath', epoch),\n",
    "#                     normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the checkpoint\n",
    "torch.save(netG.state_dict(), '%s/netG_epoch_%d.pth' % ('outpath', epoch))\n",
    "torch.save(netD.state_dict(), '%s/netD_epoch_%d.pth' % ('outpath', epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# netG = torch.load('checkpoint/netG_epoch_19.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
